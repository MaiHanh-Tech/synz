import streamlit as st
import google.generativeai as genai
import pandas as pd
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
from pypdf import PdfReader
from docx import Document
from bs4 import BeautifulSoup
import numpy as np
import os

# --- C·∫§U H√åNH ---
st.set_page_config(page_title="Mai Hanh Strategy (Pro)", layout="wide", page_icon="üíé")
st.title("üíé The Mai Hanh Analyzer (Unlimited Context)")

# --- QU·∫¢N L√ù B·∫¢O M·∫¨T (SECRETS) ---
if 'GOOGLE_API_KEY' in st.secrets:
    api_key = st.secrets['GOOGLE_API_KEY']
else:
    api_key = st.sidebar.text_input("Nh·∫≠p Google API Key:", type="password")

if not api_key:
    st.warning("‚ö†Ô∏è Vui l√≤ng nh·∫≠p API Key ƒë·ªÉ ti·∫øp t·ª•c.")
    st.stop()

genai.configure(api_key=api_key)

# *** C·∫§U H√åNH MODEL M·∫†NH NH·∫§T (LONG CONTEXT) ***
# S·ª≠ d·ª•ng 1.5 Pro v√¨ ƒë√¢y l√† b·∫£n h·ªó tr·ª£ 2 TRI·ªÜU tokens (ƒë·ªçc nguy√™n cu·ªën s√°ch)
# Google ch∆∞a c√≥ API 2.5 Pro, 1.5 Pro hi·ªán l√† b·∫£n SOTA (State-of-the-art)
try:
    model = genai.GenerativeModel('gemini-2.5-pro')
except:
    st.error("T√†i kho·∫£n ch∆∞a h·ªó tr·ª£ Pro, chuy·ªÉn v·ªÅ Flash.")
    model = genai.GenerativeModel('gemini-2.5-flash')

# --- H√ÄM X·ª¨ L√ù ---
@st.cache_resource
def load_models():
    return SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')

def doc_noi_dung_file(uploaded_file):
    if not uploaded_file: return ""
    ext = os.path.splitext(uploaded_file.name)[1].lower()
    try:
        # ƒê·ªçc PDF (To√†n b·ªô c√°c trang)
        if ext == '.pdf':
            reader = PdfReader(uploaded_file)
            return "\n".join([page.extract_text() for page in reader.pages])
        # ƒê·ªçc Word
        elif ext == '.docx':
            doc = Document(uploaded_file)
            return "\n".join([p.text for p in doc.paragraphs])
        # ƒê·ªçc Text/Markdown
        elif ext in ['.txt', '.md']:
            return str(uploaded_file.read(), "utf-8")
        # ƒê·ªçc Web/HTML
        elif ext in ['.html', '.htm']:
            soup = BeautifulSoup(uploaded_file, 'html.parser')
            text = soup.get_text()
            return text
    except Exception as e: return f"L·ªói ƒë·ªçc file: {e}"
    return ""

# --- GIAO DI·ªÜN ---
with st.sidebar:
    st.header("1. K·∫øt N·ªëi Kho S√°ch")
    file_excel = st.file_uploader("Upload Book146.xlsx", type="xlsx")
    
    vec_model = None
    db_vec = None
    df = None
    
    if file_excel:
        try:
            df = pd.read_excel(file_excel).dropna(subset=['T√™n s√°ch'])
            vec_model = load_models()
            # T·∫°o d·ªØ li·ªáu vector cho kho s√°ch
            content = [f"{r['T√™n s√°ch']} {r['C·∫¢M NH·∫¨N']}" for i,r in df.iterrows()]
            db_vec = vec_model.encode(content)
            st.success(f"‚úÖ ƒê√£ n·∫°p {len(df)} cu·ªën s√°ch c≈©.")
        except: st.error("L·ªói file Excel")

st.header("2. Upload T√†i Li·ªáu (H·ªó tr·ª£ s√°ch d√†i)")
uploaded_files = st.file_uploader(
    "K√©o th·∫£ c√°c file c·∫ßn ph√¢n t√≠ch v√†o ƒë√¢y", 
    type=["pdf","docx","txt","md","html"], 
    accept_multiple_files=True 
)

if st.button("üöÄ PH√ÇN T√çCH & T·ªîNG H·ª¢P CHI·∫æN L∆Ø·ª¢C", type="primary"):
    if not uploaded_files:
        st.warning("Ch∆∞a c√≥ file n√†o!")
    else:
        progress_bar = st.progress(0)
        total_files = len(uploaded_files)
        danh_sach_tom_tat = [] 

        st.subheader("üìù I. Ph√¢n T√≠ch Chi Ti·∫øt")
        
        for i, file_doc in enumerate(uploaded_files):
            with st.spinner(f"Gemini Pro ƒëang ƒë·ªçc to√†n b·ªô file {i+1}/{total_files}: {file_doc.name}..."):
                # 1. ƒê·ªçc n·ªôi dung (FULL - Kh√¥ng c·∫Øt)
                text = doc_noi_dung_file(file_doc)
                do_dai = len(text)
                
                # 2. RAG (T√¨m li√™n k·∫øt v·ªõi kho s√°ch c≈©)
                lien_ket = ""
                if file_excel and len(text) > 100:
                    try:
                        # Ch·ªâ l·∫•y 2000 k√Ω t·ª± ƒë·∫ßu ƒë·ªÉ t√¨m ki·∫øm vector cho nhanh
                        query_vec = vec_model.encode([text[:2000]])
                        scores = cosine_similarity(query_vec, db_vec)[0]
                        top = np.argsort(scores)[::-1][:3]
                        for idx in top:
                            if scores[idx] > 0.35: # Ng∆∞·ª°ng gi·ªëng nhau > 35%
                                lien_ket += f"- {df.iloc[idx]['T√™n s√°ch']} (T√°c gi·∫£: {df.iloc[idx]['T√°c gi·∫£']})\n"
                    except: pass

                if not lien_ket: lien_ket = "Kh√¥ng t√¨m th·∫•y li√™n k·∫øt r√µ r√†ng v·ªõi kho s√°ch c≈©."

                # 3. Prompt (G·ª≠i to√†n b·ªô n·ªôi dung s√°ch)
                # D√πng model Pro n√™n ta t·ª± tin g·ª≠i c·∫£ text d√†i
                prompt = f'''
                B·∫°n l√† Tr·ª£ l√Ω Nghi√™n c·ª©u Chi·∫øn l∆∞·ª£c (S·ª≠ d·ª•ng Model Gemini Pro - Long Context).
                
                NHI·ªÜM V·ª§: Ph√¢n t√≠ch t√†i li·ªáu: '{file_doc.name}' (ƒê·ªô d√†i: {do_dai} k√Ω t·ª±).
                
                TH√îNG TIN THAM KH·∫¢O T·ª™ KHO S√ÅCH C≈® C·ª¶A CH·ªä H·∫†NH:
                {lien_ket}
                
                Y√äU C·∫¶U: 
                1. **T√≥m t·∫Øt c·ªët l√µi:** Nh·ªØng lu·∫≠n ƒëi·ªÉm ch√≠nh y·∫øu nh·∫•t c·ªßa s√°ch/t√†i li·ªáu n√†y.
                2. **Ph√¢n t√≠ch chi·ªÅu s√¢u:** ƒê√°nh gi√° t∆∞ duy t√°c gi·∫£, ƒëi·ªÉm m·∫°nh/y·∫øu c·ªßa l·∫≠p lu·∫≠n.
                3. **K·∫øt n·ªëi tri th·ª©c:** T√†i li·ªáu n√†y b·ªï sung hay ph·∫£n bi·ªán g√¨ v·ªõi c√°c cu·ªën s√°ch c≈© trong danh s√°ch tham kh·∫£o ·ªü tr√™n?
                4. **Tr√≠ch d·∫´n ƒë·∫Øt gi√°:** 1 c√¢u tr√≠ch d·∫´n hay nh·∫•t.
                5. **Ph∆∞∆°ng ph√°p lu√¢j:** T√°c gi·∫£ ƒë√£ d√πng ph∆∞∆°ng ph√°p g√¨ ƒë·ªÉ ƒëi ƒë·∫øn k·∫øt lu·∫≠n n√†y? Gi·∫£ ƒë·ªãnh ng·∫ßm c·ªßa h·ªç l√† g√¨? N·∫øu C√ÅC t√°c gi·∫£ kh√°c  ph√¢n t√≠ch c√πng m·ªôt v·∫•n ƒë·ªÅ, H·ªå s·∫Ω n√≥i g√¨? 
                
                N·ªòI DUNG T√ÄI LI·ªÜU (FULL TEXT):
                {text}
                '''
                
                try:
                    res = model.generate_content(prompt)
                    danh_sach_tom_tat.append(f"=== T√ÄI LI·ªÜU {i+1}: {file_doc.name} ===\n{res.text}\n")
                    
                    with st.expander(f"üìÑ K·∫øt qu·∫£: {file_doc.name} (ƒê√£ ƒë·ªçc {do_dai} k√Ω t·ª±)", expanded=False):
                        st.markdown(res.text)
                except Exception as e:
                    st.error(f"L·ªói AI khi ƒë·ªçc file n√†y: {e}")
            
            progress_bar.progress((i + 1) / total_files)

        st.divider()
        st.header("üèÜ II. B√ÅO C√ÅO T·ªîNG QUAN CHI·∫æN L∆Ø·ª¢C")
        
        if len(danh_sach_tom_tat) > 0:
            with st.spinner("üß† Brain Pro ƒëang t·ªïng h·ª£p chi·∫øn l∆∞·ª£c..."):
                du_lieu_tong_hop = "\n".join(danh_sach_tom_tat)
                
                prompt_tong_hop = f'''
                B·∫°n l√† C·ªë v·∫•n Chi·∫øn l∆∞·ª£c c·∫•p cao.
                D∆∞·ªõi ƒë√¢y l√† c√°c b·∫£n ph√¢n t√≠ch c·ªßa {total_files} t√†i li·ªáu.
                
                D·ªÆ LI·ªÜU ƒê·∫¶U V√ÄO:
                {du_lieu_tong_hop}
                
                NHI·ªÜM V·ª§: Vi·∫øt B√ÅO C√ÅO T·ªîNG H·ª¢P (SYNTHESIS).
                1. **M·∫´u h√¨nh chung (Patterns):** C√°c t√†i li·ªáu n√†y c√≥ ƒëi·ªÉm g√¨ t∆∞∆°ng ƒë·ªìng v·ªÅ t∆∞ duy?
                2. **G√≥c nh√¨n ƒëa chi·ªÅu:** C√°c t√†i li·ªáu b·ªï sung hay m√¢u thu·∫´n nhau?
                3. **K·∫øt lu·∫≠n chi·∫øn l∆∞·ª£c:** B√†i h·ªçc c·ªët l√µi r√∫t ra l√† g√¨?
                
                H√£y vi·∫øt s√¢u s·∫Øc, logic.
                '''
                
                try:
                    res_tong_hop = model.generate_content(prompt_tong_hop)
                    st.success("ƒê√£ ho√†n th√†nh t·ªïng h·ª£p!")
                    st.markdown(res_tong_hop.text)
                    st.download_button("üíæ T·∫£i B√°o C√°o T·ªïng H·ª£p (.txt)", res_tong_hop.text, file_name="Bao_Cao_Tong_Hop.txt")
                except: st.error("L·ªói t·ªïng h·ª£p.")
