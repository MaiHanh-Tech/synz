import streamlit as st
import pandas as pd
import numpy as np
from datetime import datetime
from sklearn.metrics.pairwise import cosine_similarity
import time
import json

# Local imports
from services.blocks.file_processor import doc_file
from services.blocks.embedding_engine import load_encoder
from services.blocks.html_generator import load_template, create_html_block, create_interactive_html_block

# ‚úÖ S·ª¨A: Import ƒë·∫ßy ƒë·ªß t·ª´ rag_orchestrator
from services.blocks.rag_orchestrator import (
    analyze_document_streamlit, 
    compute_similarity_with_excel, 
    store_history, 
    init_knowledge_universe, 
    create_personal_rag, 
    tai_lich_su,
    get_translation_orchestrator  # ‚úÖ H√†m n√†y c√≥ ·ªü d√≤ng 356 trong rag_orchestrator.py
)

# KG module cho upgrade
from services.blocks import knowledge_graph_v2 as kg_module

# Core engines
from ai_core import AI_Core
from voice_block import Voice_Engine
from prompts import DEBATE_PERSONAS, BOOK_ANALYSIS_PROMPT
from services.blocks import knowledge_graph_v2 as kg_module

# Optional supabase import (don't fail app if missing)
try:
    from supabase import create_client, Client
except ImportError:
    pass

# TRANSLATIONS / UI TEXT
TRANS = {
    "vi": {
        "lang_select": "Ng√¥n ng·ªØ / Language / ËØ≠Ë®Ä",
        "tab1": "üìö Ph√¢n T√≠ch S√°ch",
        "tab2": "‚úçÔ∏è D·ªãch Gi·∫£",
        "tab3": "üó£Ô∏è Tranh Bi·ªán",
        "tab4": "üéôÔ∏è Ph√≤ng Thu AI",
        "tab5": "‚è≥ Nh·∫≠t K√Ω",
        "t1_header": "Tr·ª£ l√Ω Nghi√™n c·ª©u & Knowledge Graph",
        "t1_up_excel": "1. K·∫øt n·ªëi Kho S√°ch (Excel)",
        "t1_up_doc": "2. T√†i li·ªáu m·ªõi (PDF/Docx)",
        "t1_btn": "üöÄ PH√ÇN T√çCH NGAY",
        "t1_analyzing": "ƒêang ph√¢n t√≠ch {name}...",
        "t1_connect_ok": "‚úÖ ƒê√£ k·∫øt n·ªëi {n} cu·ªën s√°ch.",
        "t1_graph_title": "ü™ê V≈© tr·ª• S√°ch",
        "t1_seed_books": "‚úÖ ƒê√£ t·∫£i {n} s√°ch tinh hoa v√†o Knowledge Graph (18 s√°ch bao tr√πm 4 t·∫ßng tri·∫øt h·ªçc)",
        "t2_header": "D·ªãch Thu·∫≠t ƒêa Chi·ªÅu",
        "t2_input": "Nh·∫≠p vƒÉn b·∫£n c·∫ßn d·ªãch:",
        "t2_target": "D·ªãch sang:",
        "t2_style": "Phong c√°ch:",
        "t2_btn": "‚úçÔ∏è D·ªãch Ngay",
        "t3_header": "ƒê·∫•u Tr∆∞·ªùng T∆∞ Duy",
        "t3_persona_label": "Ch·ªçn ƒê·ªëi Th·ªß:",
        "t3_input": "Nh·∫≠p ch·ªß ƒë·ªÅ tranh lu·∫≠n...",
        "t3_clear": "üóëÔ∏è X√≥a Chat",
        "t4_header": "üéôÔ∏è Ph√≤ng Thu AI ƒêa Ng√¥n Ng·ªØ",
        "t4_voice": "Ch·ªçn Gi·ªçng:",
        "t4_speed": "T·ªëc ƒë·ªô:",
        "t4_btn": "üîä T·∫†O AUDIO",
        "t5_header": "Nh·∫≠t K√Ω & L·ªãch S·ª≠",
        "t5_refresh": "üîÑ T·∫£i l·∫°i L·ªãch s·ª≠",
        "t5_empty": "Ch∆∞a c√≥ d·ªØ li·ªáu l·ªãch s·ª≠.",
    },
    "en": {
        "lang_select": "Language",
        "tab1": "üìö Book Analysis",
        "tab2": "‚úçÔ∏è Translator",
        "tab3": "üó£Ô∏è Debater",
        "tab4": "üéôÔ∏è AI Studio",
        "tab5": "‚è≥ History",
        "t1_header": "Research Assistant & Knowledge Graph",
        "t1_up_excel": "1. Connect Book Database (Excel)",
        "t1_up_doc": "2. New Documents (PDF/Docx)",
        "t1_btn": "üöÄ ANALYZE NOW",
        "t1_analyzing": "Analyzing {name}...",
        "t1_connect_ok": "‚úÖ Connected {n} books.",
        "t1_graph_title": "ü™ê Book Universe",
        "t1_seed_books": "‚úÖ Loaded {n} foundational books into Knowledge Graph (18 books covering 4 philosophy layers)",
        "t2_header": "Multidimensional Translator",
        "t2_input": "Enter text to translate:",
        "t2_target": "Translate to:",
        "t2_style": "Style:",
        "t2_btn": "‚úçÔ∏è Translate",
        "t3_header": "Thinking Arena",
        "t3_persona_label": "Choose Opponent:",
        "t3_input": "Enter debate topic...",
        "t3_clear": "üóëÔ∏è Clear Chat",
        "t4_header": "üéôÔ∏è Multilingual AI Studio",
        "t4_voice": "Select Voice:",
        "t4_speed": "Speed:",
        "t4_btn": "üîä GENERATE AUDIO",
        "t5_header": "Logs & History",
        "t5_refresh": "üîÑ Refresh History",
        "t5_empty": "No history data found.",
    },
    "zh": {
        "lang_select": "ËØ≠Ë®Ä",
        "tab1": "üìö ‰π¶Á±çÂàÜÊûê",
        "tab2": "‚úçÔ∏è ÁøªËØë‰∏ìÂÆ∂",
        "tab3": "üó£Ô∏è Ëæ©ËÆ∫Âú∫",
        "tab4": "üéôÔ∏è AI ÂΩïÈü≥ÂÆ§",
        "tab5": "‚è≥ ÂéÜÂè≤ËÆ∞ÂΩï",
        "t1_header": "Á†îÁ©∂Âä©Êâã & Áü•ËØÜÂõæË∞±",
        "t1_up_excel": "1. ËøûÊé•‰π¶Â∫ì (Excel)",
        "t1_up_doc": "2. ‰∏ä‰º†Êñ∞ÊñáÊ°£ (PDF/Docx)",
        "t1_btn": "üöÄ Á´ãÂç≥ÂàÜÊûê",
        "t1_analyzing": "Ê≠£Âú®ÂàÜÊûê {name}...",
        "t1_connect_ok": "‚úÖ Â∑≤ËøûÊé• {n} Êú¨‰π¶„ÄÇ",
        "t1_graph_title": "ü™ê ‰π¶Á±çÂÆáÂÆô",
        "t1_seed_books": "‚úÖ Â∑≤Âä†ËΩΩ {n} Êú¨Á≤æÂçé‰π¶Á±çÂà∞Áü•ËØÜÂõæË∞± (18Êú¨‰π¶Ë¶ÜÁõñ4Â±ÇÂì≤Â≠¶)",
        "t2_header": "Â§öÁª¥ÁøªËØë",
        "t2_input": "ËæìÂÖ•ÊñáÊú¨:",
        "t2_target": "ÁøªËØëÊàê:",
        "t2_style": "È£éÊ†º:",
        "t2_btn": "‚úçÔ∏è ÁøªËØë",
        "t3_header": "ÊÄùÁª¥Á´ûÊäÄÂú∫",
        "t3_persona_label": "ÈÄâÊã©ÂØπÊâã:",
        "t3_input": "ËæìÂÖ•Ëæ©ËÆ∫‰∏ªÈ¢ò...",
        "t3_clear": "üóëÔ∏è Ê∏ÖÈô§ËÅäÂ§©",
        "t4_header": "üéôÔ∏è AI Â§öËØ≠Ë®ÄÂΩïÈü≥ÂÆ§",
        "t4_voice": "ÈÄâÊã©Â£∞Èü≥:",
        "t4_speed": "ËØ≠ÈÄü:",
        "t4_btn": "üîä ÁîüÊàêÈü≥È¢ë",
        "t5_header": "Êó•Âøó & ÂéÜÂè≤",
        "t5_refresh": "üîÑ Âà∑Êñ∞ÂéÜÂè≤",
        "t5_empty": "ÊöÇÊó†ÂéÜÂè≤Êï∞ÊçÆ„ÄÇ",
    }
}

def T(key):
    lang = st.session_state.get('weaver_lang', 'vi')
    return TRANS.get(lang, TRANS['vi']).get(key, key)

@st.cache_resource
def load_models():
    try:
        model = load_encoder()
        return model
    except Exception:
        return None

def check_model_available():
    model = load_models()
    if model is None:
        st.warning("‚ö†Ô∏è Ch·ª©c nƒÉng Knowledge Graph t·∫°m th·ªùi kh√¥ng kh·∫£ d·ª•ng (thi·∫øu RAM)")
        return False
    return True

def doc_file_safe(uploaded_file):
    return doc_file(uploaded_file)

# ‚úÖ S·ª¨A: Helper ƒë·ªÉ init KnowledgeUniverse v·ªõi s√°ch tinh hoa + Excel upgrade
@st.cache_resource
def get_knowledge_universe(excel_file=None):
    """Kh·ªüi t·∫°o Knowledge Graph v·ªõi s√°ch tinh hoa (18 s√°ch) + optional Excel upgrade"""
    try:
        # B∆Ø·ªöC 1: T·∫°o KG c∆° b·∫£n (ƒë√£ c√≥ 18 s√°ch tinh hoa t·ª´ knowledge_graph_v2.py)
        ku = init_knowledge_universe()
        if not ku:
            st.warning("‚ö†Ô∏è Kh√¥ng th·ªÉ kh·ªüi t·∫°o Knowledge Graph")
            return None
        
        # B∆Ø·ªöC 2: N·∫øu c√≥ Excel, upgrade th√™m s√°ch t·ª´ Excel
        if excel_file:
            try:
                # ƒê·ªçc Excel ƒë·ªÉ l·∫•y danh s√°ch s√°ch
                df_excel = pd.read_excel(excel_file).dropna(subset=["T√™n s√°ch"])
                st.success(f"‚úÖ ƒê√£ k·∫øt n·ªëi {len(df_excel)} cu·ªën s√°ch t·ª´ Excel")
                
                # Upgrade KG v·ªõi s√°ch t·ª´ Excel
                ku = kg_module.upgrade_existing_database(excel_file, ku)
                
                # Hi·ªÉn th·ªã th√¥ng b√°o th√†nh c√¥ng
                total_books = len(ku.graph.nodes)
                st.success(f"‚úÖ ƒê√£ t·∫£i {total_books} s√°ch v√†o Knowledge Graph (bao g·ªìm 18 s√°ch tinh hoa + {len(df_excel)} t·ª´ Excel)")
                
            except Exception as e:
                st.warning(f"‚ö†Ô∏è Kh√¥ng th·ªÉ upgrade t·ª´ Excel: {e}")
        else:
            # Ch·ªâ c√≥ s√°ch tinh hoa
            total_books = len(ku.graph.nodes)
            st.info(f"üìö ƒê√£ t·∫£i {total_books} s√°ch tinh hoa v√†o Knowledge Graph (18 s√°ch bao tr√πm 4 t·∫ßng tri·∫øt h·ªçc)")
        
        return ku
        
    except Exception as e:
        st.error(f"‚ùå L·ªói kh·ªüi t·∫°o Knowledge Graph: {e}")
        return None

# --- RUN ---
def run():
    ai = AI_Core()
    voice = Voice_Engine()

    # ‚úÖ THAY ƒê·ªîI: Kh·ªüi t·∫°o KG v·ªõi th√¥ng b√°o r√µ r√†ng v·ªÅ s√°ch tinh hoa
    knowledge_universe = get_knowledge_universe()

    with st.sidebar:
    st.markdown("---")
    st.selectbox(
        "üåê " + T("lang_select"),
        ["Ti·∫øng Vi·ªát", "English", "‰∏≠Êñá"],
        key="weaver_lang"  # Key n√†y t·ª± ƒë·ªông l∆∞u v√†o session_state, kh√¥ng c·∫ßn if-elif
    )

    st.header(f"üß† The Cognitive Weaver")
    
    # ‚úÖ HI·ªÇN TH·ªä TR·∫†NG TH√ÅI KG (M·ªöI)
    if knowledge_universe:
        summary = knowledge_universe.get_episteme_summary()
        col1, col2, col3, col4 = st.columns(4)
        for layer, data in summary.items():
            with eval(f"col{1+list(summary.keys()).index(layer)}"):
                st.metric(layer[:15], f"{data['count']} s√°ch", delta=f"{len(data['recent'])} recent")

    tab1, tab2, tab3, tab4, tab5 = st.tabs([T("tab1"), T("tab2"), T("tab3"), T("tab4"), T("tab5")])

    # TAB 1: RAG (C·∫¢I TI·∫æN v·ªõi KG integration)
    with tab1:
        st.header(T("t1_header"))
        with st.container():
            c1, c2, c3 = st.columns([1, 1, 1])
            with c1:
                file_excel = st.file_uploader(T("t1_up_excel"), type="xlsx", key="t1_excel")
            with c2:
                uploaded_files = st.file_uploader(T("t1_up_doc"), type=["pdf", "docx", "txt", "md", "html"], accept_multiple_files=True, key="t1_files")
            with c3:
                st.write("")
                st.write("")
                btn_run = st.button(T("t1_btn"), type="primary", use_container_width=True)

        # ‚úÖ RELOAD KG n·∫øu c√≥ Excel m·ªõi
        if file_excel and btn_run:
            knowledge_universe = get_knowledge_universe(file_excel)

        if btn_run and uploaded_files:
            vec = load_encoder()
            has_db_excel = bool(file_excel)

            for f in uploaded_files:
                text = doc_file_safe(f)
                if not text:
                    st.warning(f"‚ö†Ô∏è Kh√¥ng ƒë·ªçc ƒë∆∞·ª£c file {f.name}")
                    continue

                link = ""
                if has_db_excel and vec is not None:
                    try:
                        matches = compute_similarity_with_excel(text, pd.read_excel(file_excel).dropna(subset=["T√™n s√°ch"]), vec)
                        if matches:
                            link = "\n".join([f"- {m[0]} ({m[1]*100:.0f}%)" for m in matches])
                    except Exception as e:
                        st.warning(f"Kh√¥ng th·ªÉ t√≠nh similarity: {e}")

                # ‚úÖ T√åM S√ÅCH LI√äN QUAN T·ª™ KG (∆∞u ti√™n)
                related = []
                if knowledge_universe:
                    try:
                        related = knowledge_universe.find_related_books(text[:2000], top_k=5)
                    except Exception as e:
                        st.warning(f"L·ªói KG search: {e}")

                with st.spinner(T("t1_analyzing").format(name=f.name)):
                    res = analyze_document_streamlit(f.name, text, user_lang=st.session_state.get('weaver_lang', 'vi'))
                    if res and "L·ªói" not in res:
                        st.markdown(f"### üìÑ {f.name}")
                        if link:
                            st.markdown("**üîó S√°ch t∆∞∆°ng t·ª± t·ª´ Excel:**")
                            st.markdown(link)
                        if related:
                            st.markdown("**ü™ê S√°ch li√™n quan t·ª´ Knowledge Graph (18 s√°ch tinh hoa):**")
                            for node_id, title, score, explanation in related:
                                fp = knowledge_universe.graph.nodes[node_id].get("first_principles", "")
                                st.markdown(f"- **{title}** ({score:.2f}) ‚Äî {explanation}" + (f"\n  *First Principles:* {fp}" if fp else ""))
                        st.markdown(res)
                        st.markdown("---")
                        store_history("Ph√¢n T√≠ch S√°ch", f.name, res)
                    else:
                        st.error(f"‚ùå Kh√¥ng th·ªÉ ph√¢n t√≠ch file {f.name}: {res}")

        # Graph visualization (c·∫£i ti·∫øn v·ªõi KG export)
        if knowledge_universe:
            with st.expander(T("t1_graph_title"), expanded=False):
                try:
                    nodes, edges = knowledge_universe.export_for_visualization()
                    if nodes:
                        from streamlit_agraph import agraph, Node, Edge, Config
                        config = Config(width=1000, height=600, directed=True, physics=True)
                        agraph(nodes[:50], edges[:100], config)  # Limit ƒë·ªÉ tr√°nh lag
                except Exception as e:
                    st.info("üìä Graph visualization t·∫°m th·ªùi kh√¥ng kh·∫£ d·ª•ng.")
        elif file_excel:
            # Fallback Excel graph (gi·ªØ nguy√™n logic c≈©)
            try:
                if "df_viz" not in st.session_state:
                    st.session_state.df_viz = pd.read_excel(file_excel).dropna(subset=["T√™n s√°ch"])
                df_v = st.session_state.df_viz
                
                with st.expander(T("t1_graph_title"), expanded=False):
                    vec = load_models()
                    if "book_embs" not in st.session_state:
                        with st.spinner("ƒêang s·ªë h√≥a s√°ch..."):
                            st.session_state.book_embs = vec.encode(df_v["T√™n s√°ch"].tolist())
                    
                    embs = st.session_state.book_embs
                    sim = cosine_similarity(embs)
                    nodes, edges = [], []
                    
                    # Graph Config
                    total_books = len(df_v)
                    c_slider1, c_slider2 = st.columns(2)
                    with c_slider1: max_nodes = st.slider("S·ªë l∆∞·ª£ng s√°ch hi·ªÉn th·ªã:", 5, total_books, min(50, total_books))
                    with c_slider2: threshold = st.slider("ƒê·ªô t∆∞∆°ng ƒë·ªìng n·ªëi d√¢y:", 0.0, 1.0, 0.45)

                    for i in range(max_nodes):
                        nodes.append(Node(id=str(i), label=df_v.iloc[i]["T√™n s√°ch"], size=20, color="#FFD166"))
                        for j in range(i+1, max_nodes):
                            if sim[i,j]>threshold: edges.append(Edge(source=str(i), target=str(j), color="#118AB2"))
                    
                    config = Config(width=900, height=600, directed=False, physics=True, collapsible=False)
                    agraph(nodes, edges, config)
            except:
                pass

    
    # TAB 2: D·ªãch Thu·∫≠t ƒêa Chi·ªÅu (N√ÇNG C·∫§P - d√πng TranslationOrchestrator)
    with tab2:
        st.subheader(T("t2_header"))
        st.markdown("**D·ªãch vƒÉn b·∫£n ƒëa ng√¥n ng·ªØ v·ªõi phong c√°ch chuy√™n nghi·ªáp, h·ªó tr·ª£ interactive (ti·∫øng Trung) v√† t·∫£i file HTML.**")

        # Input text
        input_text = st.text_area("Nh·∫≠p vƒÉn b·∫£n c·∫ßn d·ªãch:", height=200, key="translator_input")

        col1, col2, col3 = st.columns(3)
        with col1:
            source_lang = st.selectbox(
                "Ng√¥n ng·ªØ ngu·ªìn:",
                ["Chinese", "English", "Vietnamese", "French", "Japanese", "Korean"],
                index=0
            )
        with col2:
            target_lang = st.selectbox(
                "Ng√¥n ng·ªØ ƒë√≠ch:",
                ["Vietnamese", "English", "Chinese", "French", "Japanese", "Korean"],
                index=0
            )
        with col3:
            mode = st.radio("Ch·∫ø ƒë·ªô d·ªãch:", ["Standard (HTML ƒë·∫πp)", "Interactive (ch·ªâ ti·∫øng Trung ‚Üí Vi·ªát)"], horizontal=True)

        include_english = st.checkbox("Th√™m b·∫£n d·ªãch ti·∫øng Anh l√†m tham chi·∫øu (n·∫øu c·∫ßn)", value=True)

        if st.button("üöÄ D·ªãch ngay", type="primary", use_container_width=True):
            if not input_text.strip():
                st.warning("Vui l√≤ng nh·∫≠p vƒÉn b·∫£n c·∫ßn d·ªãch.")
            else:
                # L·∫•y orchestrator
                orchestrator = get_translation_orchestrator()
                if not orchestrator:
                    st.error("‚ö†Ô∏è Kh√¥ng t·∫£i ƒë∆∞·ª£c b·ªô d·ªãch. Ki·ªÉm tra translator.py v√† API key.")
                else:
                    progress_bar = st.progress(0)
                    status_text = st.empty()

                    def update_progress(value):
                        progress_bar.progress(value)
                        status_text.text(f"ƒêang d·ªãch... {int(value*100)}%")

                    try:
                        if mode == "Interactive (ch·ªâ ti·∫øng Trung ‚Üí Vi·ªát)":
                            if source_lang != "Chinese":
                                st.error("Interactive mode ch·ªâ h·ªó tr·ª£ ngu·ªìn ti·∫øng Trung.")
                            else:
                                status_text.text("ƒêang x·ª≠ l√Ω interactive translation...")
                                html_output = orchestrator.translate_interactive(
                                    input_text,
                                    source_lang="Chinese",
                                    target_lang=target_lang
                                )
                        else:
                            status_text.text("ƒêang d·ªãch v√† t·∫°o HTML...")
                            html_output = orchestrator.translate_document(
                                input_text,
                                source_lang=source_lang,
                                target_lang=target_lang,
                                include_english=include_english and target_lang != "English",
                                progress_callback=update_progress
                            )

                        # Th√†nh c√¥ng
                        progress_bar.progress(1.0)
                        status_text.success("‚úÖ Ho√†n th√†nh!")

                        # N√∫t t·∫£i HTML
                        st.download_button(
                            label="üì• T·∫£i file HTML k·∫øt qu·∫£",
                            data=html_output.encode('utf-8'),
                            file_name=f"translation_{source_lang}_to_{target_lang}.html",
                            mime="text/html"
                        )

                        # Preview
                        with st.expander("üëÄ Xem tr∆∞·ªõc k·∫øt qu·∫£", expanded=True):
                            st.components.v1.html(html_output, height=800, scrolling=True)

                        # L∆∞u l·ªãch s·ª≠
                        store_history(
                            "D·ªãch Thu·∫≠t",
                            f"{source_lang} ‚Üí {target_lang} ({mode})",
                            input_text[:300]
                        )

                    except Exception as e:
                        progress_bar.empty()
                        status_text.empty()
                        st.error(f"‚ùå L·ªói trong qu√° tr√¨nh d·ªãch: {str(e)}")
                        with st.expander("Chi ti·∫øt l·ªói"):
                            st.exception(e)

    # TAB 3: ƒê·∫•u tr∆∞·ªùng
    with tab3:
        st.subheader(T("t3_header"))
        mode = st.radio("Mode:", ["üë§ Solo", "‚öîÔ∏è Multi-Agent"], horizontal=True, key="w_t3_mode")
        if "weaver_chat" not in st.session_state:
            st.session_state.weaver_chat = []

        if mode == "üë§ Solo":
            c1, c2 = st.columns([3, 1])
            with c1:
                persona = st.selectbox(T("t3_persona_label"), list(DEBATE_PERSONAS.keys()), key="w_t3_solo_p")
            with c2:
                if st.button(T("t3_clear"), key="w_t3_clr"):
                    st.session_state.weaver_chat = []
                    st.rerun()
            for msg in st.session_state.weaver_chat:
                st.chat_message(msg["role"]).write(msg["content"])
            if prompt := st.chat_input(T("t3_input")):
                st.chat_message("user").write(prompt)
                st.session_state.weaver_chat.append({"role": "user", "content": prompt})
                recent_history = st.session_state.weaver_chat[-10:]
                context_text = "\n".join([f"{m['role'].upper()}: {m['content']}" for m in recent_history])
                full_prompt = f"L·ªäCH S·ª¨:\n{context_text}\n\nNHI·ªÜM V·ª§: Tr·∫£ l·ªùi c√¢u h·ªèi m·ªõi nh·∫•t c·ªßa USER."
                with st.chat_message("assistant"):
                    with st.spinner("..."):
                        res = ai.generate(full_prompt, model_type="flash", system_instruction=DEBATE_PERSONAS[persona])
                        if res:
                            st.write(res)
                            st.session_state.weaver_chat.append({"role": "assistant", "content": res})
                            store_history("Tranh Bi·ªán Solo", f"{persona} - {prompt[:50]}...", f"Q: {prompt}\nA: {res}")
        else:
            participants = st.multiselect("Ch·ªçn H·ªôi ƒê·ªìng:", list(DEBATE_PERSONAS.keys()),
                                          default=[list(DEBATE_PERSONAS.keys())[0], list(DEBATE_PERSONAS.keys())[1]],
                                          max_selections=3)
            topic = st.text_input("Ch·ªß ƒë·ªÅ:", key="w_t3_topic")
            if st.button("üî• KHAI CHI·∫æN", disabled=(len(participants) < 2 or not topic)):
                st.session_state.weaver_chat = []
                start_msg = f"üì¢ **CH·ª¶ T·ªåA:** Khai m·∫°c tranh lu·∫≠n v·ªÅ: *'{topic}'*"
                st.session_state.weaver_chat.append({"role": "system", "content": start_msg})
                st.info(start_msg)
                full_transcript = [start_msg]

                MAX_DEBATE_TIME = 600
                start_time = time.time()

                with st.status("üî• Cu·ªôc chi·∫øn ƒëang di·ªÖn ra (3 v√≤ng)...") as status:
                    try:
                        for round_num in range(1, 4):
                            # ‚úÖ TH√äM: Ki·ªÉm tra timeout t·ªïng
                            elapsed = time.time() - start_time
                            if elapsed > MAX_DEBATE_TIME:
                                st.warning(f"‚è∞ H·∫øt gi·ªù! (ƒê√£ ch·∫°y {elapsed:.0f}s)")
                                break

                            status.update(label=f"üîÑ V√≤ng {round_num}/3...")

                            for i, p_name in enumerate(participants):
                                # ‚úÖ Ki·ªÉm tra timeout t·ª´ng v√≤ng
                                if time.time() - start_time > MAX_DEBATE_TIME:
                                    break

                                context_str = topic
                                if len(st.session_state.weaver_chat) > 1:
                                    recent_msgs = st.session_state.weaver_chat[-4:]
                                    context_str = "\n".join([f"{m['role']}: {m['content']}" for m in recent_msgs])

                                # ‚úÖ TƒÇNG C∆Ø·ªúNG: B·∫Øt bu·ªôc ng·∫Øn g·ªçn h∆°n
                                length_instruction = " (B·∫ÆT BU·ªòC: Tr·∫£ l·ªùi KH√îNG QU√Å 100 t·ª´. Ch·ªâ n√™u lu·∫≠n ƒëi·ªÉm ch√≠nh, kh√¥ng d√†i d√≤ng.)"

                                if round_num == 1:
                                    p_prompt = f"CH·ª¶ ƒê·ªÄ: {topic}\nNHI·ªÜM V·ª§ (V√≤ng 1): N√™u 1 quan ƒëi·ªÉm ch√≠nh + 2 l√Ω l·∫Ω. {length_instruction}"
                                else:
                                    p_prompt = f"CH·ª¶ ƒê·ªÄ: {topic}\nB·ªêI C·∫¢NH:\n{context_str}\n\nNHI·ªÜM V·ª§ (V√≤ng {round_num}): Ph·∫£n bi·ªán ng·∫Øn g·ªçn. {length_instruction}"

                                try:
                                    # ‚úÖ HI·ªÇN TH·ªä STATUS ƒêANG G·ªåI AI
                                    with st.spinner(f"ü§ñ {p_name} ƒëang suy nghƒ©..."):
                                        res = ai.generate(
                                            p_prompt,
                                            model_type="pro",
                                            system_instruction=DEBATE_PERSONAS[p_name],
                                            max_tokens=500  # ‚úÖ GI·∫¢M t·ª´ 2000 ‚Üí 500 (ng·∫Øn g·ªçn)
                                        )

                                    if res and "‚ö†Ô∏è" not in res:
                                        # L√†m s·∫°ch response
                                        clean_res = res.replace(f"{p_name}:", "").strip()
                                        clean_res = clean_res.replace(f"**{p_name}:**", "").strip()
                                        
                                        # Icon
                                        icons = {
                                            "K·∫ª Ph·∫£n Bi·ªán": "üòà",
                                            "üé© Shushu": "üé©",
                                            "üôè Ph·∫≠t T·ªï": "üôè",
                                            "ü§î Logic & Ph·∫£n Bi·ªán": "ü§î"
                                        }
                                        icon = icons.get(p_name, "ü§ñ")
                                        
                                        content_fmt = f"### {icon} {p_name}\n\n{clean_res}"
                                        st.session_state.weaver_chat.append({"role": "assistant", "content": content_fmt})
                                        full_transcript.append(content_fmt)
                                        
                                        with st.chat_message("assistant", avatar=icon):
                                            st.markdown(content_fmt)
                                        
                                        # ‚úÖ B·ªé time.sleep(5) - KH√îNG C·∫¶N CH·ªú
                                        
                                    else:
                                        st.error(f"‚ùå {p_name} kh√¥ng tr·∫£ l·ªùi ƒë∆∞·ª£c")
                                        
                                except Exception as e:
                                    st.error(f"‚ùå L·ªói g·ªçi AI cho {p_name}: {str(e)[:100]}")
                                    continue
                                    
                        status.update(label="‚úÖ Tranh lu·∫≠n k·∫øt th√∫c!", state="complete")
                        
                    except Exception as e:
                        st.error(f"‚ùå L·ªói nghi√™m tr·ªçng: {e}")

                full_log = "\n\n".join(full_transcript)
                store_history("H·ªôi ƒê·ªìng Tranh Bi·ªán", f"Ch·ªß ƒë·ªÅ: {topic}", full_log[:1000])

    # TAB 4: VOICE
    with tab4:
        st.subheader(T("t4_header"))
        
        # 1. Ch·ªçn Gi·ªçng (L·∫•y t·ª´ Voice Engine)
        if voice and hasattr(voice, 'VOICE_OPTIONS'):
            voice_opts = list(voice.VOICE_OPTIONS.keys())
            selected_voice = st.selectbox(T("t4_voice"), voice_opts, index=0)
        else:
            selected_voice = None
            st.warning("‚ö†Ô∏è Ch∆∞a t·∫£i ƒë∆∞·ª£c module gi·ªçng n√≥i.")

        # 2. Ch·ªçn T·ªëc ƒë·ªô
        speed = st.slider(T("t4_speed"), -50, 50, 0, format="%d%%")

        # 3. Nh·∫≠p vƒÉn b·∫£n
        inp_v = st.text_area("Text:", height=150)
        
        if st.button(T("t4_btn")) and inp_v:
            with st.spinner("ƒêang t·∫°o √¢m thanh..."):
                # Truy·ªÅn gi·ªçng v√† t·ªëc ƒë·ªô v√†o h√†m speak
                path = voice.speak(inp_v, voice_key=selected_voice, speed=speed)
                if path: 
                    st.audio(path)

    # TAB 5: NH·∫¨T K√ù (C√ì PH·∫¶N BAYES)
    with tab5:
        st.subheader("‚è≥ Nh·∫≠t K√Ω & Ph·∫£n Chi·∫øu T∆∞ Duy")
        if st.button("üîÑ T·∫£i l·∫°i", key="w_t5_refresh"):
            st.session_state.history_cloud = tai_lich_su()
            st.rerun()

        data = st.session_state.get("history_cloud", tai_lich_su())

        if data:
            df_h = pd.DataFrame(data)

            if "SentimentScore" in df_h.columns:
                try:
                    df_h["score"] = pd.to_numeric(df_h["SentimentScore"], errors='coerce').fillna(0)
                    import plotly.express as px
                    fig = px.line(df_h, x="Time", y="score", markers=True, color_discrete_sequence=["#76FF03"])
                    st.plotly_chart(fig, use_container_width=True)
                except:
                    pass

            with st.expander("üîÆ Ph√¢n t√≠ch T∆∞ duy theo x√°c su·∫•t Bayes (E.T. Jaynes)", expanded=False):
                st.info("AI s·∫Ω coi L·ªãch s·ª≠ ho·∫°t ƒë·ªông c·ªßa ch·ªã l√† 'D·ªØ li·ªáu quan s√°t' (Evidence) ƒë·ªÉ suy lu·∫≠n ra 'H√†m m·ª•c ti√™u' (Objective Function) v√† s·ª± d·ªãch chuy·ªÉn ni·ªÅm tin c·ªßa ch·ªã.")

                if st.button("üß† Ch·∫°y M√¥ h√¨nh Bayes ngay"):
                    with st.spinner("ƒêang t√≠nh to√°n x√°c su·∫•t h·∫≠u nghi·ªám (Posterior)..."):
                        recent_logs = df_h.tail(10).to_dict(orient="records")
                        logs_text = json.dumps(recent_logs, ensure_ascii=False)

                        bayes_prompt = f"""
                        ƒê√≥ng vai m·ªôt nh√† khoa h·ªçc t∆∞ duy theo tr∆∞·ªùng ph√°i E.T. Jaynes (s√°ch 'Probability Theory: The Logic of Science').

                        D·ªÆ LI·ªÜU QUAN S√ÅT (EVIDENCE):
                        ƒê√¢y l√† nh·∫≠t k√Ω ho·∫°t ƒë·ªông c·ªßa t√¥i:
                        {logs_text}

                        NHI·ªÜM V·ª§:
                        H√£y ph√¢n t√≠ch chu·ªói h√†nh ƒë·ªông n√†y nh∆∞ m·ªôt b√†i to√°n suy lu·∫≠n Bayes.
                        1. **X√°c ƒë·ªãnh Priors (Ni·ªÅm tin ti√™n nghi·ªám):** D·ª±a tr√™n c√°c h√†nh ƒë·ªông ƒë·∫ßu, t√¥i ƒëang quan t√¢m/tin t∆∞·ªüng ƒëi·ªÅu g√¨?
                        2. **C·∫≠p nh·∫≠t Likelihood (Kh·∫£ nƒÉng):** C√°c h√†nh ƒë·ªông ti·∫øp theo c·ªßng c·ªë hay l√†m y·∫øu ƒëi ni·ªÅm tin ƒë√≥?
                        3. **K·∫øt lu·∫≠n Posterior (H·∫≠u nghi·ªám):** Tr·∫°ng th√°i t∆∞ duy hi·ªán t·∫°i c·ªßa t√¥i ƒëang h·ªôi t·ª• v·ªÅ ƒë√¢u? C√≥ m√¢u thu·∫´n (Inconsistency) n√†o trong logic h√†nh ƒë·ªông kh√¥ng?

                        Tr·∫£ l·ªùi ng·∫Øn g·ªçn, s√¢u s·∫Øc, d√πng thu·∫≠t ng·ªØ x√°c su·∫•t nh∆∞ng d·ªÖ hi·ªÉu.
                        """

                        analysis = ai.generate(bayes_prompt, model_type="pro")
                        st.markdown(analysis)

            st.divider()
            for index, item in df_h.iterrows():
                t = str(item.get('Time', ''))
                tp = str(item.get('Type', ''))
                ti = str(item.get('Title', ''))
                ct = str(item.get('Content', ''))

                icon = "üìù"
                if "Tranh Bi·ªán" in tp:
                    icon = "üó£Ô∏è"
                elif "D·ªãch" in tp:
                    icon = "‚úçÔ∏è"

                with st.expander(f"{icon} {t} | {tp} | {ti}"):
                    st.markdown(ct)
        else:
            st.info(T("t5_empty"))
